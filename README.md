# MMNTP-Survey
Paper collection for "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey"

## Table of Contents
1. [Awesome Multimodal Tokenizers](#awesome-multimodal-tokenizers)
2. [Awesome MMNTP Models](#awesome-mmntp-models)

## Awesome Multimodal Tokenizers

### Vision 

| **Paper** | **Time** | **Modality** | **Tokenization Type** | **GitHub** |
|-----------|----------|--------------|----------|----------|
| [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020) | 2021     | Image | Continuous | [![Star](https://img.shields.io/github/stars/OpenAI/CLIP.svg?style=social&label=Star)](https://github.com/OpenAI/CLIP) |
| [Neural Discrete Representation Learning (VQVAE)](https://arxiv.org/abs/1711.00937) | 2017     | Image, Video, Audio | Discrete | -        |
| [Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification](https://arxiv.org/abs/1711.08200) | 2017     | Video | Continuous | [![Star](https://img.shields.io/github/stars/MohsenFayyaz89/T3D.svg?style=social&label=Star)](https://github.com/MohsenFayyaz89/T3D)       |


### Audio


## Awesome MMNTP Models

### Vision

| **Paper** | **Time** | **Modality** | **Model Type** | **Task** | **GitHub** |
|-----------|----------|--------------|----------------|----------|------------|
| [Randomized Autoregressive Visual Generation (RAR)](https://arxiv.org/abs/2411.00776) | 2024 | Image | Unified | Text2Image | - |
| [Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training (MonoInternVL)](https://arxiv.org/abs/2410.08202) | 2024 | Image | Unified | Image2Text | - |
| [A Single Transformer for Scalable Vision-Language Modeling (SOLO)](https://arxiv.org/abs/2407.06438) | 2024 | Image | Unified | Image2Text | - |
| [Unveiling Encoder-Free Vision-Language Models (EVE)](https://arxiv.org/abs/2406.11832) | 2024 | Image | Unified | Image2Text | - |
| [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution (Qwen2VL)](https://arxiv.org/abs/2409.12191) | 2024 | Image | Compositional | Image2Text | - |
| [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation (Janus)](https://arxiv.org/abs/2410.13848) | 2024 | Image | Compositional | Image2Text, Text2Image | - |
| [Emu3: Next-Token Prediction is All You Need (Emu3)](https://arxiv.org/abs/2409.18869) | 2024 | Image, Video | Unified | Image2Text, Text2Image, Text2Video | - |
| [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation (Show-o)](https://arxiv.org/abs/2408.12528) | 2024 | Image, Video | Unified | Image2Text, Text2Image, Text2Video | - |
| [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation (VILA-U)](https://arxiv.org/abs/2409.04429) | 2024 | Image, Video | Unified | Image2Text, Text2Image, Text2Video | - |
| [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model (Transfusion)](https://arxiv.org/abs/2408.11039) | 2024 | Image | Unified | Image2Text | - |
| [Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens (Fluid)](https://arxiv.org/abs/2410.13863) | 2024 | Image | Unified | Image2Text | - |
| [Autoregressive Image Generation without Vector Quantization (MAR)](https://arxiv.org/abs/2406.11838) | 2024 | Image | Unified | Image2Text | - |
| [Chameleon: Mixed-Modal Early-Fusion Foundation Models (Chameleon)](https://github.com/facebookresearch/chameleon) | 2024 | Image | Unified | Image2Text, Text2Image | - |
| [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models (Mini-Genimi)](https://arxiv.org/abs/2403.18814) | 2024 | Image | Compositional | Image2Text, Text2Image | - |
| [A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation (DnD-Transformer)](https://arxiv.org/abs/2410.01912) | 2024 | Image | Unified | Text2Image | - |
| [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction (VAR)](https://arxiv.org/abs/2404.02905) | 2024 | Image | Unified | Text2Image | - |
| [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation (LlamaGen)](https://arxiv.org/abs/2406.06525) | 2024 | Image | Unified | Text2Image | - |
| [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens (MiniGPT5)](https://arxiv.org/abs/2310.02239) | 2023 | Image | Compositional | Image2Text, Text2Image | - |
| [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing (Blip-Diffusion)](https://arxiv.org/abs/2305.14720) | 2023 | Image | Compositional | Text2Image | - |
| [Kosmos-G: Generating Images in Context with Multimodal Large Language Models (Kosmos-G)](https://arxiv.org/abs/2310.02992) | 2023 | Image | Compositional | Text2Image | - |
| [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization (LaVIT)](https://arxiv.org/abs/2309.04669) | 2023 | Image | Compositional | Image2Text, Text2Image | - |
| [Generative Multimodal Models are In-Context Learners (Emu2)](https://arxiv.org/abs/2312.13286) | 2023 | Image | Compositional | Image2Text, Text2Image | - |
| [Generative Pretraining in Multimodality (Emu1)](https://arxiv.org/abs/2307.05222) | 2023 | Image | Compositional | Image2Text, Text2Image | - |
| [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action (Unified-IO2)](https://arxiv.org/abs/2312.17172) | 2023 | Image, Video, Audio | Compositional | Image2Text, Text2Image, Audio2Text, Text2Audio, Text2Video | - |
| [Language Is Not All You Need: Aligning Perception with Language Models (Kosmos-1)](https://arxiv.org/abs/2302.14045) | 2023 | Image | Compositional | Image2Text | - |
| [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks (InternVL)](https://arxiv.org/abs/2312.14238) | 2023 | Image | Compositional | Image2Text | - |
| [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond (QwenVL)](https://arxiv.org/abs/2308.12966) | 2023 | Image | Compositional | Image2Text | - |
| [Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models (Molom)](https://arxiv.org/abs/2409.17146) | 2023 | Image | Compositional | Image2Text | - |
| [Fuyu-8B: A Multimodal Architecture for AI Agents (Fuyu)](https://www.adept.ai/blog/fuyu-8b) | 2023 | Image | Unified | Image2Text | - |
| [Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models (BLIP2)](https://arxiv.org/abs/2301.12597) | 2023 | Image | Compositional | Image2Text | - |
| [Visual Instruction Tuning (LLaVA)](https://arxiv.org/abs/2304.08485) | 2023 | Image | Compositional | Image2Text | - |
| [MiniGPT4: a Visual Language Model for Few-Shot Learning (MiniGPT4)](https://arxiv.org/abs/2204.14198) | 2022 | Image | Compositional | Image2Text | - |
| [Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks (Unified-IO)](https://arxiv.org/abs/2206.08916) | 2022 | Image | Compositional | Image2Text, Text2Image | - |
| [Zero-Shot Text-to-Image Generation (DALLE)](https://arxiv.org/abs/2102.12092) | 2022 | Image | Unified | Text2Image | - |
| [Flamingo: a Visual Language Model for Few-Shot Learning (Flamingo)](https://arxiv.org/abs/2204.14198) | 2022 | Image | Compositional | Image2Text | - |

### Audio
